{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "26s_g25XMRAm",
        "outputId": "94a0bdcf-796e-4cbb-c2c2-8bdd89fb9fe4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cosine Similarity between the two documents: 0.56169019227452\n"
          ]
        }
      ],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "with open('/content/doc1.txt', 'r', encoding='utf-8') as file:\n",
        "    doc1 = file.read()\n",
        "\n",
        "with open('/content/doc2.txt', 'r', encoding='utf-8') as file:\n",
        "    doc2 = file.read()\n",
        "\n",
        "documents = [doc1, doc2]\n",
        "\n",
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "tfidf_matrix = tfidf_vectorizer.fit_transform(documents)\n",
        "\n",
        "cos_sim = cosine_similarity(tfidf_matrix[0:1], tfidf_matrix)\n",
        "print(\"Cosine Similarity between the two documents:\", cos_sim[0, 1])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "TF-IDF stands for Term Frequency-Inverse Document Frequency. It's a numerical representation of a document (a piece of text) that captures the importance of each word within that document relative to a collection of documents.\n",
        "\n",
        "Term Frequency (TF): This measures how often a word appears in a document. If a word appears more frequently in a document, its TF value will be higher.\n",
        "\n",
        "Inverse Document Frequency (IDF): This measures how unique or rare a word is across all the documents in the collection. If a word appears in many documents, its IDF value will be lower."
      ],
      "metadata": {
        "id": "RPABlsMrNXT_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cosine similarity is a measure of similarity between two vectors (in this case, TF-IDF vectors). It calculates the cosine of the angle between these vectors, which represents how similar they are in direction."
      ],
      "metadata": {
        "id": "2sGAz79lN1Ft"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def jaccard_similarity(doc1, doc2):\n",
        "    set1 = set(doc1.split())\n",
        "    set2 = set(doc2.split())\n",
        "    intersection = len(set1.intersection(set2))\n",
        "    union = len(set1.union(set2))\n",
        "    return intersection / union\n",
        "\n",
        "jac_sim = jaccard_similarity(doc1, doc2)\n",
        "print(\"Jaccard Similarity between the two documents:\", jac_sim)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EwoJFEQzNYkJ",
        "outputId": "20fb58f0-04bd-4847-a54b-29a7eb346c05"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Jaccard Similarity between the two documents: 0.10454545454545454\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Jaccard similarity is a measure used to compare the similarity between two sets. In simple terms, it tells us how similar or different two sets are by considering the intersection (common elements) and union (total elements) of the sets.\n",
        "\n",
        "Imagine you have two sets, Set A and Set B. The Jaccard similarity between Set A and Set B is calculated by dividing the number of elements they have in common by the total number of unique elements in both sets."
      ],
      "metadata": {
        "id": "wWsXgQejOALg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "import nltk\n",
        "nltk.download('punkt')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7xnK5K-bRyj-",
        "outputId": "1f90bddc-a1b1-4a35-e06e-857de9bd2f70"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Baeysian Classification"
      ],
      "metadata": {
        "id": "xn3wfyqEV7gF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from collections import defaultdict\n",
        "\n",
        "df = pd.read_csv('/content/Tweets.csv')\n",
        "\n",
        "dataset = df[['text', 'airline_sentiment']]\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "word_freq = defaultdict(lambda: [0, 0])\n",
        "for _, row in dataset.iterrows():\n",
        "    text = row['text']\n",
        "    label = row['airline_sentiment']\n",
        "    words = [word.lower() for word in word_tokenize(text) if word.isalnum() and word.lower() not in stop_words]\n",
        "    for word in words:\n",
        "        word_freq[word][label == 'positive'] += 1\n",
        "\n",
        "total_positive = sum(word_freq[word][1] for word in word_freq)\n",
        "total_negative = sum(word_freq[word][0] for word in word_freq)\n",
        "prior_positive = total_positive / (total_positive + total_negative)\n",
        "prior_negative = total_negative / (total_positive + total_negative)\n",
        "\n",
        "def classify(text):\n",
        "    words = [word.lower() for word in word_tokenize(text) if word.isalnum() and word.lower() not in stop_words]\n",
        "    log_prob_positive = sum([word_freq[word][1] / total_positive for word in words])\n",
        "    log_prob_negative = sum([word_freq[word][0] / total_negative for word in words])\n",
        "    prob_positive = prior_positive * log_prob_positive\n",
        "    prob_negative = prior_negative * log_prob_negative\n",
        "    return 'positive' if prob_positive > prob_negative else 'negative'\n",
        "\n",
        "test_data = [\"@VirginAmerica seriously would pay $30 a flight for seats that didn't have this playing. it's really the only bad thing about flying VA\"]\n",
        "for text in test_data:\n",
        "    sentiment = classify(text)\n",
        "    print(f\"Sentiment of '{text}': {sentiment}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7iBM1vigPCdo",
        "outputId": "969de379-34af-480e-ab5d-fe4488f979b5"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentiment of '@VirginAmerica seriously would pay $30 a flight for seats that didn't have this playing. it's really the only bad thing about flying VA': negative\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "HERE we are giving a sample text from the dataste which is the above mentioned, then we are trying to analyse the sentiment in that statemnt.\n",
        "\n",
        "here in this example the sentiment is negative"
      ],
      "metadata": {
        "id": "FunKYLwqgYDh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bayesian Classification is a method used in machine learning for predicting the category or class of a given data point based on the probability that it belongs to each category. It's based on Bayes' theorem, which is a fundamental concept in probability theory.\n",
        "\n"
      ],
      "metadata": {
        "id": "Rj6BC0bfVGNM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#RNN"
      ],
      "metadata": {
        "id": "Ukc28Xf8WAHJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense"
      ],
      "metadata": {
        "id": "9J2sYaNVe9a2"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('/content/Tweets.csv')"
      ],
      "metadata": {
        "id": "8BmyoAtMfAv1"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "texts = df['text'].tolist()\n",
        "labels = df['airline_sentiment'].tolist()\n",
        "\n",
        "label_encoder = LabelEncoder()\n",
        "labels = label_encoder.fit_transform(labels)\n",
        "\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(texts)\n",
        "sequences = tokenizer.texts_to_sequences(texts)\n",
        "\n",
        "max_len = max(len(seq) for seq in sequences)\n",
        "sequences_padded = pad_sequences(sequences, maxlen=max_len, padding='post')\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(sequences_padded, labels, test_size=0.2, random_state=42)\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(input_dim=len(tokenizer.word_index)+1, output_dim=32, input_length=max_len))\n",
        "model.add(LSTM(32))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "model.fit(X_train, np.array(y_train), epochs=10, batch_size=1, verbose=1)\n",
        "\n",
        "loss, accuracy = model.evaluate(X_test, np.array(y_test))\n",
        "print(\"Test Loss:\", loss)\n",
        "print(\"Test Accuracy:\", accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7AFVfQK5Ut2P",
        "outputId": "f49b0403-0c8b-45e3-f0b6-883bfaebee67"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "11712/11712 [==============================] - 228s 19ms/step - loss: -0.1645 - accuracy: 0.6362\n",
            "Epoch 2/10\n",
            "11712/11712 [==============================] - 217s 18ms/step - loss: -3.2473 - accuracy: 0.6587\n",
            "Epoch 3/10\n",
            "11712/11712 [==============================] - 218s 19ms/step - loss: -2.5985 - accuracy: 0.5503\n",
            "Epoch 4/10\n",
            "11712/11712 [==============================] - 207s 18ms/step - loss: -11.9200 - accuracy: 0.6774\n",
            "Epoch 5/10\n",
            "11712/11712 [==============================] - 210s 18ms/step - loss: -23.3699 - accuracy: 0.6954\n",
            "Epoch 6/10\n",
            "11712/11712 [==============================] - 212s 18ms/step - loss: -35.6627 - accuracy: 0.7123\n",
            "Epoch 7/10\n",
            "11712/11712 [==============================] - 213s 18ms/step - loss: -51.1009 - accuracy: 0.7297\n",
            "Epoch 8/10\n",
            "11712/11712 [==============================] - 213s 18ms/step - loss: -64.3834 - accuracy: 0.7426\n",
            "Epoch 9/10\n",
            "11712/11712 [==============================] - 218s 19ms/step - loss: -83.8500 - accuracy: 0.7526\n",
            "Epoch 10/10\n",
            "11712/11712 [==============================] - 211s 18ms/step - loss: -102.5564 - accuracy: 0.7596\n",
            "92/92 [==============================] - 1s 6ms/step - loss: -63.6491 - accuracy: 0.6831\n",
            "Test Loss: -63.649131774902344\n",
            "Test Accuracy: 0.6830601096153259\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "as we can see the accuracy of the model is increasing with each epoche.\n",
        "\n",
        "also the test accuracy is about 69 percent"
      ],
      "metadata": {
        "id": "uC_k8XAXdIfB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "rnn are helpful in porcessing test data and analysing the snetiment in them"
      ],
      "metadata": {
        "id": "TCst7PogfDIf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#LSTM"
      ],
      "metadata": {
        "id": "AYNukyV1fqLX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import Sequential\n",
        "from keras.layers import LSTM, Embedding, Dense, SpatialDropout1D\n",
        "from keras.callbacks import EarlyStopping\n"
      ],
      "metadata": {
        "id": "VFjYXzNkier9"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_csv(\"/content/Tweets.csv\")"
      ],
      "metadata": {
        "id": "DVJaTFOeigE_"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = data['text']\n",
        "y = pd.get_dummies(data['airline_sentiment']).values\n",
        "\n",
        "tokenizer = Tokenizer(num_words=5000)\n",
        "tokenizer.fit_on_texts(X)\n",
        "X = tokenizer.texts_to_sequences(X)\n",
        "X = pad_sequences(X, maxlen=200)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(5000, 128, input_length=X.shape[1]))\n",
        "model.add(SpatialDropout1D(0.4))\n",
        "model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))\n",
        "model.add(Dense(3, activation='softmax'))\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "early_stop = EarlyStopping(monitor='val_loss', patience=3, verbose=1, restore_best_weights=True)\n",
        "\n",
        "history = model.fit(X_train, y_train, epochs=10, batch_size=64, validation_data=(X_test, y_test), callbacks=[early_stop])\n",
        "\n",
        "loss, accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
        "print('Test Accuracy:', accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ORZLaug7OEUp",
        "outputId": "7a953751-a976-4642-9990-1e50f73d3bb7"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "183/183 [==============================] - 160s 855ms/step - loss: 0.7165 - accuracy: 0.7018 - val_loss: 0.5323 - val_accuracy: 0.7889\n",
            "Epoch 2/10\n",
            "183/183 [==============================] - 155s 848ms/step - loss: 0.4687 - accuracy: 0.8155 - val_loss: 0.5003 - val_accuracy: 0.8046\n",
            "Epoch 3/10\n",
            "183/183 [==============================] - 145s 791ms/step - loss: 0.3733 - accuracy: 0.8613 - val_loss: 0.5163 - val_accuracy: 0.8029\n",
            "Epoch 4/10\n",
            "183/183 [==============================] - 144s 787ms/step - loss: 0.3188 - accuracy: 0.8794 - val_loss: 0.5359 - val_accuracy: 0.8026\n",
            "Epoch 5/10\n",
            "183/183 [==============================] - ETA: 0s - loss: 0.2774 - accuracy: 0.8993Restoring model weights from the end of the best epoch: 2.\n",
            "183/183 [==============================] - 140s 767ms/step - loss: 0.2774 - accuracy: 0.8993 - val_loss: 0.5650 - val_accuracy: 0.7978\n",
            "Epoch 5: early stopping\n",
            "Test Accuracy: 0.8046448230743408\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "for the lstm model we introduce an early stop for epoch to prevent overfitting of the model"
      ],
      "metadata": {
        "id": "omL7hNWPoD21"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "the test accuracy is about 89percent"
      ],
      "metadata": {
        "id": "j6a9MrmJpN6B"
      }
    }
  ]
}